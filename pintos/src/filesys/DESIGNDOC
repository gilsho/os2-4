       	 +-------------------------+
		     | CS 140                  |
		     | PROJECT 4: FILE SYSTEMS |
		     | DESIGN DOCUMENT         |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Gil Shotan <gilsho@stanford.edu>
Rory MacQueen <macqueen@stanford.edu>
Dan Cocuzzo <dcocuzzo@stanford.edu>


---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Describe briefly which parts of the assignment were implemented by
>> each member of your team. If some team members contributed significantly
>> more or less than others (e.g. 2x), indicate that here.

All parts were implemented together with equal contribution.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

Our inode structure contains 12 direct sector pointers, 1 indirect block
pointer, and 1 doubly-indirect pointer. The indirect block contains 128 
direct sector pointers. The doubly-indirect block contains 128 indirect 
block pointers, which each contain 128 sector pointers. In total, there 
are 12 + 128 + 128^2 data sectors. Since each data sector is 512 bytes, 
the maximum size of a file is: 

(12 + 128 + 128^2) * (512) = 8460288 bytes = 8262 KB


If we include the overheard of allocating metadata, there are 130 
additionalsectors (1 for the inode, 129 for indirect pointer tables), 
which brings the total filesize to 8526848 bytes.

---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

To determine if a process needs to extend a file, it must compare
its 'desired' length to the current length of the file. Before 
getting the current length of a file, a process must acquire the 
lock for that file. If the process determines that an extension
is necessary, it first checks to see if the file is already being
extended. If it is, then the current process must wait on a condition 
variable until the previous extension is complete. Upon waking, it 
will determine again if an extension is necessary.

When a process extends, it releases the lock, and allocates the 
sectors needed for extension. It then writes its data to those
sectors. Once allocation and writing of the extended region is 
complete, the process reacquires the inode metadata lock and 
updates the file length to reflect the extension. The key point
is that the file length is updated only after the allocation and
writing of data is complete. It will then broadcast to other
processes who are waiting to extend.


------------------------------------------------------------------

Synchronization is enforced at the inode level for both files and
directories. Each inode contains a flag indicating if an extension
is in process and a condition variable associated with that extension 
flag. Read and write operations do not hold the inode during execution, 
however the lock must be acquired briefly to examine an inode's metadata 
(the extending flag and file length).

Only one process may extend a file at any given time. The kernel detects 
if an inode extension is necessary by examining the length of the inode
after a write operation. If an extension is necessary and the extension 
flag is clear, the process sets the extension flag, releases the lock,
and proceeds to allocate and write the extended region. If the process
attempts to extend but cannot because another is extending, the process 
waits on a condition variable until the previous extension is complete. 
Upon waking, it will determine again if an extension is necessary.

Once allocation and writing of the extended region is complete, the 
process reacquires the inode metadata lock and updates the file length 
to reflect the extension. This allows other processes  to read and write
(but not extend) the inode concurrently because they will not observe 
the change in file length until the extension is complete and therefore
will not attempt to access the extended file region.


>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.


Our implementation of file extension guarantees that from the perspective
of process A, file extension by process B is atomic. This is because
process B will only update the file length after the entire allocation and
writing of new sectors is complete. Process A will, therefore, either get 
the 'old' inode length, in which case he is unable to read data that is 
currently being written by process B, or, he gets the new inode length,
in which case process B has completed its writes, so process A can now read
all of the newly extended file's data blocks.

Since process B will synchronously update the file length
only after the extension (allocation and writing) is complete, process A 
will read all or none of the extended file region based the file length
it observed while briefly acquiring the inode metadata lock.


>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

The only blocking that happens is a file extension blocking 
another file extension. If a process wants to extend the file,
it must wait until all previous extensions are complete.

No reader or writer who does not need to extend is ever blocked 
when accessing a file so there is no possibility of readers 
or writers indefinitely blocking each other.

None of these operations hold the lock while reading or writing
data. The lock is only held breifly while checking inode metadata.


---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

We adopt the multilevel index scheme presented in lecture on our
inode structure, which contains direct, indirect, and doubly
idirect blocks. Given that the filesystem capacity is at most 8 MB,
we chose a combination that would give us a max file size of 8MB. 
Having 1 indirect sector and 1 double indirect sector allows us to
meet this size requirement. 

We also wanted to make sure we had good performace on small files
since the majority of files are relatively small in practice. Having
12 direct sector pointers means that the sectors for the first 
6 KB of a file can be accessed in just one seek.

We include a single sector for indirect pointer entries by convention, 
as well as for improved seek times (two seeks) versus doubly indirect
accesses (3 seeks).

			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

Given a valid user-specified path, we first determine if it is an 
absolute path or a relative path by examining the first character;
a leading '/' indicates it will be parsed as an absolute path. If it
is an absolute path, the traversal begins in the root directory; it 
it is a relative path, traversal begins in the process' current working
directory.

Our path traversal algorithm is an iterative approach. We tokenize 
the full path using the '/' delimiter. For each token, we check to 
see if an entry whose name is token exists in the current directory.
If it does not, then we can simply return false, since the path could
not be resolved. If it is, then the current directory becomes the token
just found, and we repeat the procedure with the next token. Once we
run out of tokens, we can return the inode that we have found. If at any
point we try to open an entry as a directory, and it was NOT a directory
we also return false, since this means that the path was mal-formed.


---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

Whenever a process wants to add or remove a file, it must first
acquire the lock for the parent directory. It then checks to
see if that file has already been created/removed. If so, then
it returns from the function. If not, then it can proceed with
its operation, and release the lock only after it has
created/removed the file. In this way, the check for the file's
existence and the creation/removal are together an atomic 
operation. Therefore, we cannot get race conditions on our
directory entries. 


>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process' current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

Our implementation allows for empty directories to be removed, even 
if it is the current process' working directory or if it is in use 
by another process. However, this prevents any process from
creating files in this directory since you are not allowed to 
create a file in a directory whose inode has been set to be 
removed. Since this directory is by definition empty (only empty
directories can be deleted) no process could read from this 
directory either. 


---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

A process' current working directory is stored as a struct dir on its 
thread structure. We chose to store the working directory on
the thread struct because all threads, including the main thread,
are intialized through the function. This is where inherit the parent
process' working directory, or set the working directory to root for
the main thread.


			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

Our cache replacement is an implementation of LRU eviction. We maintain
a static list of cache entries that are not blocking on I/O, ordered
by least to most recently accessed time. When sectors that have existing
entries in the cache are accessed due to read/write operations, they are
move to the tail of the list. When eviction occurs, the LRU algorithm 
simply pops the entry at the front of the list, which is the least 
recently used entry. The new entry that is put in its place is appended 
to the tail of the LRU entry list.

>> C3: Describe your implementation of write-behind.

When the cache is initialized, a dedicated 'cleaner' thread is
spawned. This thread runs in a continuous while loop. It sleeps for 30
seconds, then wakes up and flushes the entire cache by writing all contents
to disk. It repeats this procedure for the entire duration of the program.

>> C4: Describe your implementation of read-ahead.

Our read ahead implementation consists of one thread running in the 
background and a queue of read ahead requests. When reading or writing 
to a file, call cache_fetch with the sector number of the next sector in 
the file. cache_fetch appends the sector number to the back of the fetch 
queue and wakes up the thread in charge of performing the read ahead. The 
read ahead thread, when woken up, places the contents of the requested 
sectors in the cache. The thread continues processing all requests and goes 
to sleep waiting for more requests to arrive. Access to the request queue is 
synchronized using a dedicated lock.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

Firstly, since the eviction policy is LRU, once a block is about to 
be read from or written to, it is placed to the back of the queue of blocks
eligible for eviction. Hence, a block that is actively being read
from or written to is unlikely to be chosen for eviction.

When a process is about to read from or write to a buffer cache block,
that process increments the 'num_accessors' variable on that cache block.
If another process decides to evict a certain block, it must wait until the 
number of accessors for that block is zero. Thus it is blocked while other 
processes are actively reading from and writing to the sector. Note, however
that having to wait here is highly unlikely and, indeed, will only happen 
when every block in the cache is being used by some process.


>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

During eviction of a block, we set a flag on that block 'io-busy' to
be true. Whenever a process tries to access a block it must check the
io-busy flag. If it is set to true, the process must wait until the
eviction is complete. 

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

Buffer caching:
Repetitive accesses of the same memory location will benefit from
buffer caching, e.g. frequently changing the state of some character
in a game

Read-ahead:
Sequential access of a file would benefit from read-ahead. For 
example playing a music or video file, where you know you are
going to want to read later blocks in the file.

Write-behind:
Any workload which wanted to not lose all its data after an unexpected
system crash would benefit from write-behind, because the process could
be assured that at most, it only lost the most recent 30 seconds of 
data.


			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?